Can be seen as a residual attention that is applied to the outputs of the usual attention mechanism. 
![[Pasted image 20230815235712.png]]
###### Tags
#MachineLearning #NeuralNetworks #AttentionMechanism #GeometricDeepLearning  #DeepLearning #TransformerArchitecture #AlphaFold