Let $p$ and $q$ be distributions over some set.
The cross entropy of $q$ relative to $p$ is:
$$
H(p, q) = -\mathbb{E}_p(\log q)
$$
The cross entropy of a distribution with itself is its [[entropy]].
Alternatively, we can write:
$$
H(p,q) = H(p, p) + D_{KL}(p\|q)
$$
(See: [[Kullback-Leibler Divergence]])

Can be interpreted as the average number of bits needed to #Pending 
See: [[Kraft-McMillan Inequality]]
